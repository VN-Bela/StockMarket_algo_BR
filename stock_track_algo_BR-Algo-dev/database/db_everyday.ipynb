{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, MetaData, Table\n",
    "import time\n",
    "from datetime import datetime\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db_engine(user, password, host_name, database_name):\n",
    "    \"\"\"\n",
    "    Creates a connection to the MySQL database.\n",
    "    \"\"\"\n",
    "    init_engine = create_engine(f'mysql+mysqlconnector://{user}:{password}@{host_name}/{database_name}')\n",
    "    return init_engine\n",
    "\n",
    "def connect_to_engine(engine):\n",
    "    conn = engine.connect()\n",
    "    return conn\n",
    "\n",
    "user_name = \"postgres\"\n",
    "user_password = \"manager\"\n",
    "host = \"localhost\"\n",
    "port = \"5432\"\n",
    "database_name = \"temp1\"\n",
    "\n",
    "# PostgreSQL connection string\n",
    "postgres_connection_string = f\"postgresql://{user_name}:{user_password}@{host}:{port}/{database_name}\"\n",
    "\n",
    "# Create the connection to the database\n",
    "engine = create_engine(postgres_connection_string)\n",
    "\n",
    "conn = connect_to_engine(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in the database: dict_keys(['RELIANCE', '^NSEI', '^NSEBANK', 'TATAMOTORS', 'IREDA'])\n"
     ]
    }
   ],
   "source": [
    "# Reflect the database tables\n",
    "metadata = MetaData()\n",
    "metadata.reflect(bind=engine)\n",
    "\n",
    "table_names_list = metadata.tables.keys()\n",
    "\n",
    "print(f\"Tables in the database: {table_names_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerun_fetch_data(symbol, date):\n",
    "\n",
    "    if symbol == \"^NSEI\" or symbol == \"^NSEBANK\":\n",
    "        symbol = symbol.upper()\n",
    "    else:\n",
    "        symbol = f\"{symbol}.NS\".upper()\n",
    "\n",
    "    date = datetime.strptime(date, '%Y-%m-%d %H:%M:%S')\n",
    "    # temp_end = datetime.strptime(\"2024-01-31\", '%Y-%m-%d')\n",
    "\n",
    "    # Download 5-minute data\n",
    "    min5 = yf.download(symbol, start=date, interval='5m')\n",
    "    min5.index = min5.index.strftime('%Y-%m-%d %H:%M:%S') \n",
    "    min5.index = pd.to_datetime(min5.index)\n",
    "    min5['Date'] = min5.index.date\n",
    "    for i in range(len(min5.columns)):\n",
    "        min5.columns.values[i] = \"5m_\" + min5.columns.values[i]\n",
    "    min5.rename(columns={'5m_Date':'Date'}, inplace=True)\n",
    "    min5 = min5.reindex(columns=['Date','5m_Open', '5m_High', '5m_Low', '5m_Close', '5m_Adj Close', '5m_Volume'])\n",
    "\n",
    "    # Download 15-minute data\n",
    "    min15 = yf.download(symbol, start=date, interval='15m')\n",
    "    min15.index = min15.index.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    min15.index = pd.to_datetime(min15.index)\n",
    "    min15['Date'] = min15.index.date\n",
    "    for i in range(len(min15.columns)):\n",
    "        min15.columns.values[i] = \"15m_\" + min15.columns.values[i]\n",
    "    min15.rename(columns={'15m_Date':'Date'}, inplace=True)\n",
    "    min15 = min15.reindex(columns=['Date','15m_Open', '15m_High', '15m_Low', '15m_Close', '15m_Adj Close', '15m_Volume'])\n",
    "    min15 = min15.resample('5T').ffill()\n",
    "\n",
    "    # Download 60-minute data\n",
    "    min60 = yf.download(symbol, start=date, interval='60m')\n",
    "    min60.index = min60.index.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    min60.index = pd.to_datetime(min60.index)\n",
    "    min60['Date'] = min60.index.date\n",
    "    for i in range(len(min60.columns)):\n",
    "        min60.columns.values[i] = \"60m_\" + min60.columns.values[i]\n",
    "    min60.rename(columns={'60m_Date':'Date'}, inplace=True)\n",
    "    min60 = min60.reindex(columns=['Date','60m_Open', '60m_High', '60m_Low', '60m_Close', '60m_Adj Close', '60m_Volume'])\n",
    "    min60 = min60.resample('5T').ffill()\n",
    "\n",
    "    # Download 60-minute data\n",
    "    day1 = yf.download(symbol, start=date, interval='1d')\n",
    "    day1.index = day1.index.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    day1.index = pd.to_datetime(day1.index)\n",
    "    day1['Date'] = day1.index.date\n",
    "    for i in range(len(day1.columns)):\n",
    "        day1.columns.values[i] = \"1d_\" + day1.columns.values[i]\n",
    "    day1.rename(columns={'1d_Date':'Date'}, inplace=True)\n",
    "    day1 = day1.reindex(columns=['Date','1d_Open', '1d_High', '1d_Low', '1d_Close', '1d_Adj Close', '1d_Volume'])\n",
    "\n",
    "    # Extend the index to the end of the day\n",
    "    last_timestamp = day1.index[-1]\n",
    "    end_of_day = last_timestamp.replace(hour=23, minute=59, second=59)\n",
    "    new_index = pd.date_range(start=day1.index[0], end=end_of_day, freq='T')\n",
    "    day1 = day1.reindex(new_index)\n",
    "\n",
    "    day1 = day1.resample('5T').ffill()\n",
    "    day1.fillna(method='ffill', inplace=True)\n",
    "\n",
    "\n",
    "    # Concatenate dataframes\n",
    "    main_df = pd.concat([min5, min15, min60, day1], axis=1)\n",
    "    # Drop duplicate columns\n",
    "    main_df = main_df.loc[:, ~main_df.columns.duplicated()]\n",
    "    # pd.set_option('display.max_rows', None)\n",
    "    main_df = main_df.dropna(subset=['5m_Open','5m_High','5m_Low','5m_Close'])\n",
    "    \n",
    "    return main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2646"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_sql_table('IREDA', con=engine)\n",
    "\n",
    "\n",
    "# Find the starting index of the latest chunk of consecutive null-filled values\n",
    "null_chunks = df['1d_Open'].isnull().astype(int).diff().fillna(0)\n",
    "latest_chunk_start_index = null_chunks[null_chunks == 1].index[-1]\n",
    "\n",
    "len(null_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-23 09:15:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-23 09:15:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-23 09:15:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-23 09:15:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-23 09:15:00\n",
      "                Datetime                 Date     5m_Open     5m_High  \\\n",
      "0    2024-03-28 09:15:00  2024-03-28 00:00:00  138.350006  139.300003   \n",
      "1    2024-03-28 09:20:00  2024-03-28 00:00:00  138.250000  138.500000   \n",
      "2    2024-03-28 09:25:00  2024-03-28 00:00:00  138.050003  138.149994   \n",
      "3    2024-03-28 09:30:00  2024-03-28 00:00:00  137.199997  137.399994   \n",
      "4    2024-03-28 09:35:00  2024-03-28 00:00:00  137.399994  137.399994   \n",
      "...                  ...                  ...         ...         ...   \n",
      "2642 2024-05-23 10:40:00           2024-05-23  190.500000  190.750000   \n",
      "2643 2024-05-23 10:45:00           2024-05-23  190.399994  190.649994   \n",
      "2644 2024-05-23 10:50:00           2024-05-23  190.500000  191.000000   \n",
      "2645 2024-05-23 10:55:00           2024-05-23  190.750000  191.300003   \n",
      "2646 2024-05-23 11:00:00           2024-05-23  190.949997  190.949997   \n",
      "\n",
      "          5m_Low    5m_Close  5m_Adj Close  5m_Volume    15m_Open    15m_High  \\\n",
      "0     137.800003  138.300003    138.300003        0.0  138.350006  139.300003   \n",
      "1     137.800003  138.149994    138.149994   429779.0  138.350006  139.300003   \n",
      "2     137.100006  137.250000    137.250000   281113.0  138.350006  139.300003   \n",
      "3     136.350006  137.350006    137.350006   541093.0  137.199997  138.000000   \n",
      "4     137.050003  137.300003    137.300003   167655.0  137.199997  138.000000   \n",
      "...          ...         ...           ...        ...         ...         ...   \n",
      "2642  189.449997  190.399994    190.399994   897548.0  191.899994  192.250000   \n",
      "2643  189.899994  190.399994    190.399994   411062.0  190.399994  191.300003   \n",
      "2644  190.350006  190.649994    190.649994   372613.0  190.399994  191.300003   \n",
      "2645  190.399994  190.949997    190.949997   300646.0  190.399994  191.300003   \n",
      "2646  190.949997  190.949997    190.949997        0.0  190.949997  190.949997   \n",
      "\n",
      "      ...     60m_Low  60m_Close  60m_Adj Close  60m_Volume     1d_Open  \\\n",
      "0     ...  136.350006     138.25         138.25         0.0  138.250000   \n",
      "1     ...  136.350006     138.25         138.25         0.0  138.250000   \n",
      "2     ...  136.350006     138.25         138.25         0.0  138.250000   \n",
      "3     ...  136.350006     138.25         138.25         0.0  138.250000   \n",
      "4     ...  136.350006     138.25         138.25         0.0  138.250000   \n",
      "...   ...         ...        ...            ...         ...         ...   \n",
      "2642  ...         NaN        NaN            NaN         NaN  189.199997   \n",
      "2643  ...         NaN        NaN            NaN         NaN  189.199997   \n",
      "2644  ...         NaN        NaN            NaN         NaN  189.199997   \n",
      "2645  ...         NaN        NaN            NaN         NaN         NaN   \n",
      "2646  ...         NaN        NaN            NaN         NaN         NaN   \n",
      "\n",
      "         1d_High      1d_Low    1d_Close  1d_Adj Close   1d_Volume  \n",
      "0     139.500000  135.500000  135.899994    135.899994  13307678.0  \n",
      "1     139.500000  135.500000  135.899994    135.899994  13307678.0  \n",
      "2     139.500000  135.500000  135.899994    135.899994  13307678.0  \n",
      "3     139.500000  135.500000  135.899994    135.899994  13307678.0  \n",
      "4     139.500000  135.500000  135.899994    135.899994  13307678.0  \n",
      "...          ...         ...         ...           ...         ...  \n",
      "2642  194.600006  187.199997  191.000000    191.000000  25414848.0  \n",
      "2643  194.600006  187.199997  191.000000    191.000000  25414848.0  \n",
      "2644  194.600006  187.199997  191.000000    191.000000  25414848.0  \n",
      "2645         NaN         NaN         NaN           NaN         NaN  \n",
      "2646         NaN         NaN         NaN           NaN         NaN  \n",
      "\n",
      "[2647 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "for table_name in table_names_list:\n",
    "\n",
    "    df = pd.read_sql_table(table_name, con=engine)\n",
    "\n",
    "    # Find the starting index of the latest chunk of consecutive null-filled values\n",
    "    null_chunks = df['1d_Open'].isnull().astype(int).diff().fillna(0)\n",
    "    latest_chunk_start_index = null_chunks[null_chunks == 1].index[-1]\n",
    "\n",
    "    date_value = df.loc[latest_chunk_start_index, 'Datetime'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    proper_df = df[df['Datetime'] < date_value]\n",
    "\n",
    "\n",
    "    # print the last row datetime value of the proper_df\n",
    "    null_values_df = df[df['Datetime'] >= date_value]\n",
    "\n",
    "\n",
    "    datetime_value = df.loc[null_values_df.index[0], 'Datetime']\n",
    "\n",
    "    date_value = datetime_value.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    after_df = rerun_fetch_data(table_name, str(datetime_value))\n",
    "\n",
    "    # convert the index of after_df as Datetime column\n",
    "    after_df['Datetime'] = after_df.index\n",
    "\n",
    "    after_df = after_df.reindex(columns=['Datetime','Date','5m_Open', '5m_High', '5m_Low', '5m_Close', '5m_Adj Close', '5m_Volume',\n",
    "                              '15m_Open', '15m_High', '15m_Low', '15m_Close', '15m_Adj Close', '15m_Volume',\n",
    "                              '60m_Open', '60m_High', '60m_Low', '60m_Close', '60m_Adj Close', '60m_Volume',\n",
    "                              '1d_Open', '1d_High', '1d_Low', '1d_Close', '1d_Adj Close', '1d_Volume'])\n",
    "    \n",
    "    after_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    final_df = pd.concat([proper_df, after_df])\n",
    "\n",
    "    final_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    final_df.iloc[-2:, final_df.columns.get_indexer(['1d_Open', '1d_High', '1d_Low', '1d_Close', '1d_Adj Close', '1d_Volume'])] = None\n",
    "\n",
    "\n",
    "    final_df.to_csv(f'final_{table_name}.csv')\n",
    "\n",
    "    # Delete the existing table\n",
    "    table_to_delete = Table(table_name, metadata)\n",
    "    table_to_delete.drop(engine)\n",
    "\n",
    "    final_df.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "    # after_df.to_csv(f'{table_name}.csv')\n",
    "\n",
    "\n",
    "\n",
    "    print(datetime_value)\n",
    "\n",
    "\n",
    "print(final_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
